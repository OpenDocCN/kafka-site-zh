<!--~
  ~ Licensed to the Apache Software Foundation (ASF) under one or more
  ~ contributor license agreements.  See the NOTICE file distributed with
  ~ this work for additional information regarding copyright ownership.
  ~ The ASF licenses this file to You under the Apache License, Version 2.0
  ~ (the "License"); you may not use this file except in compliance with
  ~ the License.  You may obtain a copy of the License at
  ~
  ~    http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  ~-->

<script id="connect-template" type="text/x-handlebars-template">
    <h3><a id="connect_overview" href="#connect_overview">8.1 概述</a></h3>

    <p>Kafka Connect 是一款可扩展并且可靠地在 Apache Kafka 和其他系统之间进行数据传输的工具。 可以很简单的快速定义 <i>connectors</i> 将大量数据从 Kafka 移入和移出. Kafka Connect 可以摄取数据库数据或者收集应用程序的 metrics 存储到 Kafka topics，使得数据可以用于低延迟的流处理。 一个导出的 job 可以将来自 Kafka topic 的数据传输到二级存储，用于系统查询或者批量进行离线分析。</p>

    <p>Kafka Connect 功能包括:</p>
    <ul>
        <li><b>Kafka connectors 通用框架：</b> - Kafka Connect 将其他数据系统和Kafka集成标准化,简化了 connector 的开发,部署和管理</li>
        <li><b>分布式和单机模式</b> - 可以扩展成一个集中式的管理服务，也可以单机方便的开发,测试和生产环境小型的部署。</li>
        <li><b>REST 接口</b> - submit and manage connectors to your Kafka Connect cluster via an easy to use REST API</li>
        <li><b>offset 自动管理</b> - 只需要connectors 的一些信息，Kafka Connect 可以自动管理offset 提交的过程，因此开发人员无需担心开发中offset提交出错的这部分。</li>
        <li><b>分布式的并且可扩展</b> - Kafka Connect 构建在现有的 group 管理协议上。Kafka Connect 集群可以扩展添加更多的workers。</li>
        <li><b>整合流处理/批处理</b> - 利用 Kafka 已有的功能，Kafka Connect 是一个桥接stream 和批处理系统理想的方式。</li>
    </ul>

    <h3><a id="connect_user" href="#connect_user">8.2 用户指南</a></h3>

    <p> quickstart 提供了一个简单的例子，演示如何运行一个单机版的Kafka Connect。 这一节描述如何配置，如何管理Kafka Connect 的更多细节。</p>

    <h4><a id="connect_running" href="#connect_running">运行 Kafka Connect</a></h4>

    <p>Kafka Connect 当前支持两种执行方式: 单机 (单个进程) 和 分布式.</p>

    <p>在单机模式下所有的工作都是在一个进程中运行的。connect的配置项很容易配置和开始使用，当只有一台机器(worker)的时候也是可用的(例如，收集日志文件到kafka)，但是不利于Kafka Connect 的容错。你可以通过下面的命令启动一个单机进程:</p>

    <pre class="brush: bash;">
    &gt; bin/connect-standalone.sh config/connect-standalone.properties connector1.properties [connector2.properties ...]
    </pre>

    <p>第一个参数是 worker 的配置文件. 其中包括 Kafka connection 参数，序列化格式，和如何频繁的提交offsets。 所提供的示例可以在本地良好的运行，使用默认提供的配置 <code>config/server.properties</code> 。它需要调整以配合不同的配置或生产环境部署。所有的workers（独立和分布式）都需要一些配置 :</p>
    <ul>
        <li><code>bootstrap.servers</code> - List of Kafka servers used to bootstrap connections to Kafka</li>
        <li><code>key.converter</code> - Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the keys in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro.</li>
        <li><code>value.converter</code> - Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the values in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro.</li>
    </ul>

    <p>单机模式的重要配置如下:</p>
    <ul>
        <li><code>offset.storage.file.filename</code> - 存储 offset 数据的文件</li>
    </ul>

    <p>此处配置的参数适用于由Kafka Connect使用的 producer 和 consumer 访问配置，offset 和 status topic。对于 Kafka source和 sink 任务的配置，可以使用相同的参数，但必须以<code>consumer.</code> 和 <code>producer.</code> 作为前缀。 此外，从 worker 配置中继承的参数只有一个，就是 <code>bootstrap.servers</code>。大多数情况下，这是足够的，因为同一个集群通常用于所有的场景。但是需要注意的是一个安全集群，需要额外的参数才能允许连接。这些参数需要在 worker 配置中设置三次，一次用于管理访问，一次用于 Kafka sinks，还有一次用于 Kafka source。</p>

    <p>其余参数用于 connector 的配置文件，你可以导入尽可能多的配置文件，但是所有的配置文件都将在同一个进程内(在不同的线程上)执行。</p>

    <p>分布式模式下会自动进行负载均衡，允许动态的扩缩容，并提供对 active task，以及这个任务对应的配置和offset提交记录的容错。分布式执行方式和单机模式非常相似:</p>

    <pre class="brush: bash;">
    &gt; bin/connect-distributed.sh config/connect-distributed.properties
    </pre>

    <p>和单机模式不同在于启动的实现类和决定 Kafka connect 进程如何工作的配置参数，如何分配 work,offsets 存储在哪里和任务状态。在分布式模式中，Kafka Connect 存储 offsets,配置和存储在 Kafka topic中的任务状态。建议手动创建Kafka 的 offsets,配置和状态，以实现自己所期望的分区数和备份因子。如果启动Kafka Connect之前没有创建 topic，则会使用默认分区数和复制因子自动创建创建 topic，但这可能不是最适合的。</p>

    <p>特别是，除了上面提到的常用设置之外，以下配置参数在启动集群之前至关重要:</p>
    <ul>
        <li><code>group.id</code> (default <code>connect-cluster</code>) - unique name for the cluster, used in forming the Connect cluster group; note that this <b>must not conflict</b> with consumer group IDs</li>
        <li><code>config.storage.topic</code> (default <code>connect-configs</code>) - topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated, compacted topic. You may need to manually create the topic to ensure the correct configuration as auto created topics may have multiple partitions or be automatically configured for deletion rather than compaction</li>
        <li><code>offset.storage.topic</code> (default <code>connect-offsets</code>) - topic to use for storing offsets; this topic should have many partitions, be replicated, and be configured for compaction</li>
        <li><code>status.storage.topic</code> (default <code>connect-status</code>) - topic to use for storing statuses; this topic can have multiple partitions, and should be replicated and configured for compaction</li>
    </ul>

    <p>注意在分布式模式下 connector 配置不会通过命令行传递。相反，会使用下面提到的 REST API来创建，修改和销毁 connectors。</p>


    <h4><a id="connect_configuring" href="#connect_configuring">Configuring Connectors</a></h4>

    <p>Connector 配置是简单的key-value 映射的格式。对于单机模式，这些配置会在 properties 文件中定义，并通过命令行传递给 Connect 进程。在分布式模式中，它们将被包含在创建（或修改）connector 的请求的JSON格式串中。</p>

    <p>大多数配置都依赖于 connectors,所以在这里不能概述。但是，有几个常见选项可以看一下:</p>

    <ul>
        <li><code>name</code> - Unique name for the connector. Attempting to register again with the same name will fail.</li>
        <li><code>connector.class</code> - The Java class for the connector</li>
        <li><code>tasks.max</code> - The maximum number of tasks that should be created for this connector. The connector may create fewer tasks if it cannot achieve this level of parallelism.</li>
        <li><code>key.converter</code> - (optional) Override the default key converter set by the worker.</li>
        <li><code>value.converter</code> - (optional) Override the default value converter set by the worker.</li>
    </ul>

    <p> <code>connector.class</code> 配置支持多种名称格式：这个 connector class 的全名或者别名。如果 connector 是 org.apache.kafka.connect.file.FileStreamSinkConnector，则可以指定全名，也可以使用FileStreamSink 或 FileStreamSinkConnector 来简化配置。</p>

    <p>Sink connectors 还有一个额外的选项来控制他的输出:</p>
    <ul>
        <li><code>topics</code> - A list of topics to use as input for this connector</li>
    </ul>

    <p>对于任何其他选项，你应该查阅 connector的文档.</p>

    <h4><a id="connect_transforms" href="#connect_transforms">Transformations</a></h4>

    <p>connectors可以配置 transformations 操作，实现轻量级的消息单次修改，他们可以方便地用于数据修改和事件路由。</p>

    <p>A transformation chain 可以在connector 配置中指定。</p>

    <ul>
        <li><code>transforms</code> - List of aliases for the transformation, specifying the order in which the transformations will be applied.</li>
        <li><code>transforms.$alias.type</code> - Fully qualified class name for the transformation.</li>
        <li><code>transforms.$alias.$transformationSpecificConfig</code> Configuration properties for the transformation</li>
    </ul>

    <p>例如，让我们使用内置的 file soucre connector，并使用 transformation 来添加静态字段。</p>

    <p>这个例子中，我们会使用 schemaless json 数据格式。为了使用 schemaless 格式，我们将 <code>connect-standalone.properties</code>  文件中下面两行从true改成false:</p>

    <pre class="brush: text;">
        key.converter.schemas.enable
        value.converter.schemas.enable
    </pre>

    <p>这个 file source connector 读取每行数据作为一个字符串。我们会将每行数据包装进一个 Map 数据结构,然后添加一个二级字段来标识事件的来源。做这样一个操作，我们使用两个 transformations:</p>
    <ul>
        <li><b>HoistField</b> to place the input line inside a Map</li>
        <li><b>InsertField</b> to add the static field. In this example we'll indicate that the record came from a file connector</li>
    </ul>

    <p>添加完 transformations, <code>connect-file-source.properties</code> 文件像下面这样:</p>

    <pre class="brush: text;">
        name=local-file-source
        connector.class=FileStreamSource
        tasks.max=1
        file=test.txt
        topic=connect-test
        transforms=MakeMap, InsertSource
        transforms.MakeMap.type=org.apache.kafka.connect.transforms.HoistField$Value
        transforms.MakeMap.field=line
        transforms.InsertSource.type=org.apache.kafka.connect.transforms.InsertField$Value
        transforms.InsertSource.static.field=data_source
        transforms.InsertSource.static.value=test-file-source
    </pre>

    <p>所有以<code>transforms</code> 为开头的行都将被添加了静态字段用于 transformations 。 你可以看到我们创建的两个 transformations: "InsertSource" 和 "MakeMap" 是我们给的 transformations 的别称. transformation 类型基于下面给的一系列内嵌 transformations。每个 transformation 类型都有额外的配置: HoistField 需要一个配置叫做 "field”,这是 map中原始字符串的字段名称。InsertField transformation 让我们指定字段名称和我们要添加的内容。</p>

    <p>当我们对一个 sample file 运行 file source connector 操作，不做transformations 操作，然后使用<code>kafka-console-consumer.sh</code> 读取数据，结果如下:</p>

    <pre class="brush: text;">
        "foo"
        "bar"
        "hello world"
   </pre>

    <p>然后我们创建一个新的file connector,然后将这个transformations 添加到配置文件中。这次结果如下:</p>

    <pre class="brush: json;">
        {"line":"foo","data_source":"test-file-source"}
        {"line":"bar","data_source":"test-file-source"}
        {"line":"hello world","data_source":"test-file-source"}
    </pre>

    <p>你可以看到我们读取的行现在JSON map的一部分，并且还有一个静态值的额外字段。这只是用 transformations 做的一个简单的例子。.</p>

    <p>Kafka Connect 包含几个广泛适用的数据和 routing transformations </p>

    <ul>
        <li>InsertField - Add a field using either static data or record metadata</li>
        <li>ReplaceField - Filter or rename fields</li>
        <li>MaskField - Replace field with valid null value for the type (0, empty string, etc)</li>
        <li>ValueToKey</li>
        <li>HoistField - Wrap the entire event as a single field inside a Struct or a Map</li>
        <li>ExtractField - Extract a specific field from Struct and Map and include only this field in results</li>
        <li>SetSchemaMetadata - modify the schema name or version</li>
        <li>TimestampRouter - Modify the topic of a record based on original topic and timestamp. Useful when using a sink that needs to write to different tables or indexes based on timestamps</li>
        <li>RegexRouter - modify the topic of a record based on original topic, replacement string and a regular expression</li>
    </ul>

    <p>如何配置每个transformation，参考下面: </p>

    <div id="org.apache.kafka.connect.transforms.InsertField">
<h5>org.apache.kafka.connect.transforms.InsertField</h5>
Insert field(s) using attributes from the record metadata or a configured static value.<p/>Use the concrete transformation type designed for the record key (<code>org.apache.kafka.connect.transforms.InsertField$Key</code>) or value (<code>org.apache.kafka.connect.transforms.InsertField$Value</code>).
<p/>
<table class="data-table"><tbody>
<tr>
<th>Name</th>
<th>Description</th>
<th>Type</th>
<th>Default</th>
<th>Valid Values</th>
<th>Importance</th>
</tr>
<tr>
<td>offset.field</td><td>Field name for Kafka offset - only applicable to sink connectors.<br/>Suffix with <code>!</code> to make this a required field, or <code>?</code> to keep it optional (the default).</td><td>string</td><td>null</td><td></td><td>medium</td></tr>
<tr>
<td>partition.field</td><td>Field name for Kafka partition. Suffix with <code>!</code> to make this a required field, or <code>?</code> to keep it optional (the default).</td><td>string</td><td>null</td><td></td><td>medium</td></tr>
<tr>
<td>static.field</td><td>Field name for static data field. Suffix with <code>!</code> to make this a required field, or <code>?</code> to keep it optional (the default).</td><td>string</td><td>null</td><td></td><td>medium</td></tr>
<tr>
<td>static.value</td><td>Static field value, if field name configured.</td><td>string</td><td>null</td><td></td><td>medium</td></tr>
<tr>
<td>timestamp.field</td><td>Field name for record timestamp. Suffix with <code>!</code> to make this a required field, or <code>?</code> to keep it optional (the default).</td><td>string</td><td>null</td><td></td><td>medium</td></tr>
<tr>
<td>topic.field</td><td>Field name for Kafka topic. Suffix with <code>!</code> to make this a required field, or <code>?</code> to keep it optional (the default).</td><td>string</td><td>null</td><td></td><td>medium</td></tr>
</tbody></table>
</div>
<div id="org.apache.kafka.connect.transforms.ReplaceField">
<h5>org.apache.kafka.connect.transforms.ReplaceField</h5>
Filter or rename fields.<p/>Use the concrete transformation type designed for the record key (<code>org.apache.kafka.connect.transforms.ReplaceField$Key</code>) or value (<code>org.apache.kafka.connect.transforms.ReplaceField$Value</code>).
<p/>
<table class="data-table"><tbody>
<tr>
<th>Name</th>
<th>Description</th>
<th>Type</th>
<th>Default</th>
<th>Valid Values</th>
<th>Importance</th>
</tr>
<tr>
<td>blacklist</td><td>Fields to exclude. This takes precedence over the whitelist.</td><td>list</td><td>""</td><td></td><td>medium</td></tr>
<tr>
<td>renames</td><td>Field rename mappings.</td><td>list</td><td>""</td><td>list of colon-delimited pairs, e.g. <code>foo:bar,abc:xyz</code></td><td>medium</td></tr>
<tr>
<td>whitelist</td><td>Fields to include. If specified, only these fields will be used.</td><td>list</td><td>""</td><td></td><td>medium</td></tr>
</tbody></table>
</div>
<div id="org.apache.kafka.connect.transforms.MaskField">
<h5>org.apache.kafka.connect.transforms.MaskField</h5>
Mask specified fields with a valid null value for the field type (i.e. 0, false, empty string, and so on).<p/>Use the concrete transformation type designed for the record key (<code>org.apache.kafka.connect.transforms.MaskField$Key</code>) or value (<code>org.apache.kafka.connect.transforms.MaskField$Value</code>).
<p/>
<table class="data-table"><tbody>
<tr>
<th>Name</th>
<th>Description</th>
<th>Type</th>
<th>Default</th>
<th>Valid Values</th>
<th>Importance</th>
</tr>
<tr>
<td>fields</td><td>Names of fields to mask.</td><td>list</td><td></td><td>non-empty list</td><td>high</td></tr>
</tbody></table>
</div>
<div id="org.apache.kafka.connect.transforms.ValueToKey">
<h5>org.apache.kafka.connect.transforms.ValueToKey</h5>
Replace the record key with a new key formed from a subset of fields in the record value.
<p/>
<table class="data-table"><tbody>
<tr>
<th>Name</th>
<th>Description</th>
<th>Type</th>
<th>Default</th>
<th>Valid Values</th>
<th>Importance</th>
</tr>
<tr>
<td>fields</td><td>Field names on the record value to extract as the record key.</td><td>list</td><td></td><td>non-empty list</td><td>high</td></tr>
</tbody></table>
</div>
<div id="org.apache.kafka.connect.transforms.HoistField">
<h5>org.apache.kafka.connect.transforms.HoistField</h5>
Wrap data using the specified field name in a Struct when schema present, or a Map in the case of schemaless data.<p/>Use the concrete transformation type designed for the record key (<code>org.apache.kafka.connect.transforms.HoistField$Key</code>) or value (<code>org.apache.kafka.connect.transforms.HoistField$Value</code>).
<p/>
<table class="data-table"><tbody>
<tr>
<th>Name</th>
<th>Description</th>
<th>Type</th>
<th>Default</th>
<th>Valid Values</th>
<th>Importance</th>
</tr>
<tr>
<td>field</td><td>Field name for the single field that will be created in the resulting Struct or Map.</td><td>string</td><td></td><td></td><td>medium</td></tr>
</tbody></table>
</div>
<div id="org.apache.kafka.connect.transforms.ExtractField">
<h5>org.apache.kafka.connect.transforms.ExtractField</h5>
Extract the specified field from a Struct when schema present, or a Map in the case of schemaless data. Any null values are passed through unmodified.<p/>Use the concrete transformation type designed for the record key (<code>org.apache.kafka.connect.transforms.ExtractField$Key</code>) or value (<code>org.apache.kafka.connect.transforms.ExtractField$Value</code>).
<p/>
<table class="data-table"><tbody>
<tr>
<th>Name</th>
<th>Description</th>
<th>Type</th>
<th>Default</th>
<th>Valid Values</th>
<th>Importance</th>
</tr>
<tr>
<td>field</td><td>Field name to extract.</td><td>string</td><td></td><td></td><td>medium</td></tr>
</tbody></table>
</div>
<div id="org.apache.kafka.connect.transforms.SetSchemaMetadata">
<h5>org.apache.kafka.connect.transforms.SetSchemaMetadata</h5>
Set the schema name, version or both on the record's key (<code>org.apache.kafka.connect.transforms.SetSchemaMetadata$Key</code>) or value (<code>org.apache.kafka.connect.transforms.SetSchemaMetadata$Value</code>) schema.
<p/>
<table class="data-table"><tbody>
<tr>
<th>Name</th>
<th>Description</th>
<th>Type</th>
<th>Default</th>
<th>Valid Values</th>
<th>Importance</th>
</tr>
<tr>
<td>schema.name</td><td>Schema name to set.</td><td>string</td><td>null</td><td></td><td>high</td></tr>
<tr>
<td>schema.version</td><td>Schema version to set.</td><td>int</td><td>null</td><td></td><td>high</td></tr>
</tbody></table>
</div>
<div id="org.apache.kafka.connect.transforms.TimestampRouter">
<h5>org.apache.kafka.connect.transforms.TimestampRouter</h5>
Update the record's topic field as a function of the original topic value and the record timestamp.<p/>This is mainly useful for sink connectors, since the topic field is often used to determine the equivalent entity name in the destination system(e.g. database table or search index name).
<p/>
<table class="data-table"><tbody>
<tr>
<th>Name</th>
<th>Description</th>
<th>Type</th>
<th>Default</th>
<th>Valid Values</th>
<th>Importance</th>
</tr>
<tr>
<td>timestamp.format</td><td>Format string for the timestamp that is compatible with <code>java.text.SimpleDateFormat</code>.</td><td>string</td><td>yyyyMMdd</td><td></td><td>high</td></tr>
<tr>
<td>topic.format</td><td>Format string which can contain <code>${topic}</code> and <code>${timestamp}</code> as placeholders for the topic and timestamp, respectively.</td><td>string</td><td>${topic}-${timestamp}</td><td></td><td>high</td></tr>
</tbody></table>
</div>
<div id="org.apache.kafka.connect.transforms.RegexRouter">
<h5>org.apache.kafka.connect.transforms.RegexRouter</h5>
Update the record topic using the configured regular expression and replacement string.<p/>Under the hood, the regex is compiled to a <code>java.util.regex.Pattern</code>. If the pattern matches the input topic, <code>java.util.regex.Matcher#replaceFirst()</code> is used with the replacement string to obtain the new topic.
<p/>
<table class="data-table"><tbody>
<tr>
<th>Name</th>
<th>Description</th>
<th>Type</th>
<th>Default</th>
<th>Valid Values</th>
<th>Importance</th>
</tr>
<tr>
<td>regex</td><td>Regular expression to use for matching.</td><td>string</td><td></td><td>valid regex</td><td>high</td></tr>
<tr>
<td>replacement</td><td>Replacement string.</td><td>string</td><td></td><td></td><td>high</td></tr>
</tbody></table>
</div>
<div id="org.apache.kafka.connect.transforms.Flatten">
<h5>org.apache.kafka.connect.transforms.Flatten</h5>
Flatten a nested data structure, generating names for each field by concatenating the field names at each level with a configurable delimiter character. Applies to Struct when schema present, or a Map in the case of schemaless data. The default delimiter is '.'.<p/>Use the concrete transformation type designed for the record key (<code>org.apache.kafka.connect.transforms.Flatten$Key</code>) or value (<code>org.apache.kafka.connect.transforms.Flatten$Value</code>).
<p/>
<table class="data-table"><tbody>
<tr>
<th>Name</th>
<th>Description</th>
<th>Type</th>
<th>Default</th>
<th>Valid Values</th>
<th>Importance</th>
</tr>
<tr>
<td>delimiter</td><td>Delimiter to insert between field names from the input record when generating field names for the output record</td><td>string</td><td>.</td><td></td><td>medium</td></tr>
</tbody></table>
</div>
<div id="org.apache.kafka.connect.transforms.Cast">
<h5>org.apache.kafka.connect.transforms.Cast</h5>
Cast fields or the entire key or value to a specific type, e.g. to force an integer field to a smaller width. Only simple primitive types are supported -- integers, floats, boolean, and string. <p/>Use the concrete transformation type designed for the record key (<code>org.apache.kafka.connect.transforms.Cast$Key</code>) or value (<code>org.apache.kafka.connect.transforms.Cast$Value</code>).
<p/>
<table class="data-table"><tbody>
<tr>
<th>Name</th>
<th>Description</th>
<th>Type</th>
<th>Default</th>
<th>Valid Values</th>
<th>Importance</th>
</tr>
<tr>
<td>spec</td><td>List of fields and the type to cast them to of the form field1:type,field2:type to cast fields of Maps or Structs. A single type to cast the entire value. Valid types are int8, int16, int32, int64, float32, float64, boolean, and string.</td><td>list</td><td></td><td>list of colon-delimited pairs, e.g. <code>foo:bar,abc:xyz</code></td><td>high</td></tr>
</tbody></table>
</div>
<div id="org.apache.kafka.connect.transforms.TimestampConverter">
<h5>org.apache.kafka.connect.transforms.TimestampConverter</h5>
Convert timestamps between different formats such as Unix epoch, strings, and Connect Date/Timestamp types.Applies to individual fields or to the entire value.<p/>Use the concrete transformation type designed for the record key (<code>org.apache.kafka.connect.transforms.TimestampConverter$Key</code>) or value (<code>org.apache.kafka.connect.transforms.TimestampConverter$Value</code>).
<p/>
<table class="data-table"><tbody>
<tr>
<th>Name</th>
<th>Description</th>
<th>Type</th>
<th>Default</th>
<th>Valid Values</th>
<th>Importance</th>
</tr>
<tr>
<td>target.type</td><td>The desired timestamp representation: string, unix, Date, Time, or Timestamp</td><td>string</td><td></td><td></td><td>high</td></tr>
<tr>
<td>field</td><td>The field containing the timestamp, or empty if the entire value is a timestamp</td><td>string</td><td>""</td><td></td><td>high</td></tr>
<tr>
<td>format</td><td>A SimpleDateFormat-compatible format for the timestamp. Used to generate the output when type=string or used to parse the input if the input is a string.</td><td>string</td><td>""</td><td></td><td>medium</td></tr>
</tbody></table>
</div>

    <!--#include virtual="generated/connect_transforms.html" -->

    <h4><a id="connect_rest" href="#connect_rest">REST API</a></h4>

    <p>由于Kafka  Connect 旨在作为服务运行，它还提供了一个用于管理 connectors 的REST API。sssss默认情况下，此服务在端口8083上运行。以下是当前支持的功能:
    </p>

    <ul>
        <li><code>GET /connectors</code> - return a list of active connectors</li>
        <li><code>POST /connectors</code> - create a new connector; the request body should be a JSON object containing a string <code>name</code> field and an object <code>config</code> field with the connector configuration parameters</li>
        <li><code>GET /connectors/{name}</code> - get information about a specific connector</li>
        <li><code>GET /connectors/{name}/config</code> - get the configuration parameters for a specific connector</li>
        <li><code>PUT /connectors/{name}/config</code> - update the configuration parameters for a specific connector</li>
        <li><code>GET /connectors/{name}/status</code> - get current status of the connector, including if it is running, failed, paused, etc., which worker it is assigned to, error information if it has failed, and the state of all its tasks</li>
        <li><code>GET /connectors/{name}/tasks</code> - get a list of tasks currently running for a connector</li>
        <li><code>GET /connectors/{name}/tasks/{taskid}/status</code> - get current status of the task, including if it is running, failed, paused, etc., which worker it is assigned to, and error information if it has failed</li>
        <li><code>PUT /connectors/{name}/pause</code> - pause the connector and its tasks, which stops message processing until the connector is resumed</li>
        <li><code>PUT /connectors/{name}/resume</code> - resume a paused connector (or do nothing if the connector is not paused)</li>
        <li><code>POST /connectors/{name}/restart</code> - restart a connector (typically because it has failed)</li>
        <li><code>POST /connectors/{name}/tasks/{taskId}/restart</code> - restart an individual task (typically because it has failed)</li>
        <li><code>DELETE /connectors/{name}</code> - delete a connector, halting all tasks and deleting its configuration</li>
    </ul>

    <p>Kafka Connect还提供用于获取有关 connector plugin sss信息的REST API:</p>

    <ul>
        <li><code>GET /connector-plugins</code>- return a list of connector plugins installed in the Kafka Connect cluster. Note that the API only checks for connectors on the worker that handles the request, which means you may see inconsistent results, especially during a rolling upgrade if you add new connector jars</li>
        <li><code>PUT /connector-plugins/{connector-type}/config/validate</code> - validate the provided configuration values against the configuration definition. This API performs per config validation, returns suggested values and error messages during validation.</li>
    </ul>

    <h3><a id="connect_development" href="#connect_development">8.3 Connector 开发者指南</a></h3>

    <p>本指南介绍开发人员如何为 Kafka connector 编写新的 connectors，用于Kafka和其他系统之间移动数据。</p>

    <h4><a id="connect_concepts" href="#connect_concepts">Core Concepts 和 APIs</a></h4>

    <h5><a id="connect_connectorsandtasks" href="#connect_connectorsandtasks">Connectors 和 Tasks</a></h5>
    <code>HDFSSinkConnector</code>
    <p>要在Kafka和另一个系统之间复制数据，用户会为想要 pull 数据或者 push 数据的系统创建一个<code>connector</code>。 connector 有两类:<code>SourceConnectors</code> 从其他系统导入数据(e.g.<code>JDBCSourceConnector</code> 会将关系型数据库导入到Kafka中)和<code>SinkConnectors</code>导出数据(e.g. <code>HDFSSinkConnector</code>会将Kafka topic 的内容导出到 HDFS 文件)</p>

    <p><code>Connectors</code> 自身不执行任何数据复制:<code>Connector</code>的配置描述要复制的数据，并且<code>Connector</code> 负责负责将 job 分解为可分发给 worker 的一组 <code>Tasks</code>。这些<code>Tasks</code>也分为两类: <code>SourceTask</code> 和 <code>SinkTask</code>。</p>

    <p>通过分配，每个<code>Task</code> 必须将数据的一部分子集复制到Kafka或者从Kafka复制。在 Kafka Connect中，应该始终可以将这些分配的数据框架化为一组输入和输出流，这些流由具有一致结构的记录组成。有的时候这些映射是显而易见的:一组日志文件中的每个文件都认为是一个流，每条解析的行数据使用相同的模式和偏移量作为字节偏移量存储在文件中。在其他情况下，可能需要花费更多功夫来映射模型: 一个 JDBC connector可以将表映射成stream，但是offset不能确定。可以使用时间戳字段进行增量查询返回新的数据，最后查询的时间戳可以用作偏移量。</p>


    <h5><a id="connect_streamsandrecords" href="#connect_streamsandrecords">Streams 和 Records</a></h5>

    <p>每个stream 都应该是一串 key-value的就。keys和values可以有复杂的数据结构-提供了很多基本类型，arrays，objects和嵌套的数据结构。每个stream 都应该是一串 key-value的就。keys和values可以有复杂的数据结构-提供了很多基本类型，也可以用来表示arrays，objects和嵌套的数据结构。运行时的数据格式不承担任何特定的序列化格式:此转换由框架内部处理。</p>

    <p>除了 key 和 value, records(sources生成的记录和发送到sinks的记录) 关联 stream IDs和offsets。框架会定期的提交已经处理数据的offsets，以便在发生故障时，可以从最后一次提交的offsets恢复处理，避免不必要的重新处理和重复事件。.</p>

    <h5><a id="connect_dynamicconnectors" href="#connect_dynamicconnectors">Dynamic Connectors</a></h5>

    <p>不是所有的jobs都是静态，所以<code>Connector</code> 的实现还要负责监控外部系统是否需要重新更改配置。例如，在<code>JDBCSourceConnector</code>的例子中，这个<code>Connector</code>可能分配一组 tables 给 <code>Task</code>。当一个新的 table 创建了，必须发现这个时间，以便通过更新配置来将新表分配给其中一个<code>Tasks</code>。当发现需要重新配置的变更(或者<code>Tasks</code> 数量)的时候，他会通知框架，并更新相应的<code>Tasks</code>。</p>


    <h4><a id="connect_developing" href="#connect_developing">开发一个简单的 Connector</a></h4>

    <p>开发一个 connector 只需要实现两个接口,  <code>Connector</code> 和 <code>Task</code>接口. 一个简单的例子的源码在Kafka<code>file</code>  package中。 connector 用于单机模式，并拥有 <code>SourceConnector</code> 和<code>SourceTask</code>实现来读取一个文件的每行记录，并将其作为记录发发送，<code>SinkConnector</code>的<code>SinkTask</code>将记录写入到文件。</p>

    <p>本节的其余部分将通过一些代码演示创建 connector 的关键步骤，但开发人员还应参考完整的示例源代码，因为为简洁起见，省略了许多细节。</p>

    <h5><a id="connect_connectorexample" href="#connect_connectorexample">Connector Example</a></h5>

    <p>首先我们用一个简单的例子介绍一下 <code>SourceConnector</code>，<code>SinkConnector</code> 的实现和它非常相似。首先创建一个继承自 <code>SourceConnector</code> 的类，并添加一些字段来存储解析出的配置信息(要读取的文件名以及要发送数据的topic):</p>

    <pre class="brush: java;">
    public class FileStreamSourceConnector extends SourceConnector {
        private String filename;
        private String topic;
    </pre>

    <p>最简单的方法是<code>taskClass()</code>，它定义了应该在 worker 进程中实例化以实际读取数据的类:</p>

    <pre class="brush: java;">
    @Override
    public Class&lt;? extends Task&gt; taskClass() {
        return FileStreamSourceTask.class;
    }
    </pre>

    <p>我们会在后面定义 <code>FileStreamSourceTask</code> 类。接下来，我们要往<code>FileStreamSourceConnector</code>类中添加一些标准的生命周期方法。</p>:

    <pre class="brush: java;">
    @Override
    public void start(Map&lt;String, String&gt; props) {
        // The complete version includes error handling as well.
        filename = props.get(FILE_CONFIG);
        topic = props.get(TOPIC_CONFIG);
    }

    @Override
    public void stop() {
        // Nothing to do since no background monitoring is required.
    }
    </pre>

    <p>最后，实现的真正核心是 <code>taskConfigs()</code>。因此即使允许我们按照<code>maxTasks</code>参数创建更多的 task，我们也只返回包含一个 entry 的 list：</p>

    <pre class="brush: java;">
    @Override
    public List&lt;Map&lt;String, String&gt;&gt; taskConfigs(int maxTasks) {
        ArrayList&lt;Map&lt;String, String&gt;&gt; configs = new ArrayList&lt;&gt;();
        // Only one input stream makes sense.
        Map&lt;String, String&gt; config = new HashMap&lt;&gt;();
        if (filename != null)
            config.put(FILE_CONFIG, filename);
        config.put(TOPIC_CONFIG, topic);
        configs.add(config);
        return configs;
    }
    </pre>

    <p>尽管这个示例中没有用到，但 <code> SourceTask </code> 还提供了两个用于提交源系统中的 offset 的 API：<code> commit </code>和<code> commitRecord </code>。这些 API 是为具有消息确认机制的源系统提供的。覆盖这些方法允许 source connector 在源系统中的消息被写入 Kafka 后，批量或单独确认消息。
                <code> commit </code> API 将偏移量存储在源系统中，<code> poll </code>方法每次返回的偏移量都会被存储起来。这个API 的实现应该一直处于阻塞状态直到提交完成。在每个<code> SourceRecord </code>被写入 Kafka 之后，<code> commitRecord </code> API 会将其偏移量保存在源系统中。由于 Kafka Connect 会自动记录偏移量，因此不需要<code> SourceTask </code>来实现它们。在 connector 确实需要确认源系统中的消息的情况下，通常只需要其中一个API。</p>

    <p>即使是多任务, 这个方法实现通常也很简单。它只需要确定 input task 的数量，这可能需要联系它从中拉取数据的远程服务，然后由这些 task 来分摊工作。由于这些多个 task 分摊工作的的模式非常普遍，因为在 <code>ConnectorUtils</code> 工具类中提供了一些实用程序来简化这些模式。</p>

    <p>注意，这个简单的例子没有包括动态输入。有关如何触发 task 配置的更新，请参阅下一节的讨论。</p>

    <h5><a id="connect_taskexample" href="#connect_taskexample">Task Example - Source Task</a></h5>

    <p>接下来我们将描述相应的 <code>SourceTask</code> 的实现。 这里的实现很简短, 但如果要想完全涵盖本指南的内容就太长了。我们将用伪代码描述大多数实现，你也可以参考完整示例的源代码。</p>

    <p>就像 Connector 一样，我们需要创建一个类并继承对应的基类<code>Task</code>。 它也有一些标准的生命周期方法：</p>


    <pre class="brush: java;">
    public class FileStreamSourceTask extends SourceTask {
        String filename;
        InputStream stream;
        String topic;

        @Override
        public void start(Map&lt;String, String&gt; props) {
            filename = props.get(FileStreamSourceConnector.FILE_CONFIG);
            stream = openOrThrowError(filename);
            topic = props.get(FileStreamSourceConnector.TOPIC_CONFIG);
        }

        @Override
        public synchronized void stop() {
            stream.close();
        }
    </pre>

    <p>这是一个轻量级的简化版本，但表明这些方法应该相对简单，并且它们唯一要做的就是分配和释放资源。 关于这个实现有两点要注意： 第一， <code>start()</code> 方法没有处理从之前的 offset 恢复的情形，这一点将在后面的章节中讨论。第二， <code>stop()</code> 方法是同步的。这是必须的，因为 <code>SourceTasks</code> 有一个专用的线程，可以无限期的阻塞下去，所以它们需要通过 Worker 中另一个线程的调用来停止。</p>

    <p>接下来，我们要实现 task 的主要功能，<code>poll()</code>方法负责从输入系统获取事件并返回一个<code>List&lt;SourceRecord&gt;</code>：</p>

    <pre class="brush: java;">
    @Override
    public List&lt;SourceRecord&gt; poll() throws InterruptedException {
        try {
            ArrayList&lt;SourceRecord&gt; records = new ArrayList&lt;&gt;();
            while (streamValid(stream) &amp;&amp; records.isEmpty()) {
                LineAndOffset line = readToNextLine(stream);
                if (line != null) {
                    Map&lt;String, Object&gt; sourcePartition = Collections.singletonMap("filename", filename);
                    Map&lt;String, Object&gt; sourceOffset = Collections.singletonMap("position", streamOffset);
                    records.add(new SourceRecord(sourcePartition, sourceOffset, topic, Schema.STRING_SCHEMA, line));
                } else {
                    Thread.sleep(1);
                }
            }
            return records;
        } catch (IOException e) {
            // Underlying stream was killed, probably as a result of calling stop. Allow to return
            // null, and driving thread will handle any shutdown if necessary.
        }
        return null;
    }
    </pre>

    <p>同样，我们也省略了一些细节，但是我们可以看到重要的步骤：<code>poll()</code>方法会被反复调用，并且对于每次调用，它会循环尝试从文件中读取记录。对于它读取的每一行，它也会跟踪文件偏移量。它使用这些信息创建一个输出 <code>SourceRecord</code>，带有四部分信息：source partition（本示例中只有一个，即正在读取的单个文件），source offset（文件中的字节偏移量），output topic name 和 output value（读取的行，并且我们使用一个模式指定该值始终是一个字符串）。 SourceRecord构造函数的其他变体还可以包含特定的输出分区和密钥。</p>

    <p>.注意，这个实现使用了普通的Java <code>InputStream</code>接口，没有可用数据时会休眠。这是可以接受的，因为 Kafka 为每个 task 提供了一个专用线程。虽然 task 的实现必须覆盖基本的 <code>poll()</code> 接口，但实现起来有很大的灵活性。在这种情况下，基于 NIO 的实现将更加高效，但这种简单的方法很有效，实现起来很快，并且与旧版本的 Java 兼容。</p>

    <h5><a id="connect_sinktasks" href="#connect_sinktasks">Sink Tasks</a></h5>

    <p>之前的部分描述了如何实现一个简单的<code>SourceTask</code>。 与 <code>SourceConnector</code> 和 <code>SinkConnector</code>不同的是，<code>SourceTask</code> 和 <code>SinkTask</code> 有着完全不同的接口：<code>SourceTask</code> 使用 pull 接口， <code>SinkTask</code> 使用 push 接口。两者共享相同的生命周期方法，但 <code>SinkTask</code> 接口有点特殊：</p>

    <pre class="brush: java;">
    public abstract class SinkTask implements Task {
        public void initialize(SinkTaskContext context) {
            this.context = context;
        }

        public abstract void put(Collection&lt;SinkRecord&gt; records);

        public void flush(Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets) {
        }
    </pre>

    <p><code>SinkTask</code> 的文档包含了所有的细节，但这个接口几乎和 <code>SourceTask</code>一样简单。<code>put()</code>方法应包含了大部分实现，包括接受 <code>SinkRecords</code> 集合，以及执行任何必需的转换，并将它们存储在目标系统中。此方法无需确保数据在返回之前已完全写入目标系统。实际上，在许多情况下 internal buffering 会很有用，使得我们可以一次发送整批记录，从而减少将事件插入下游数据存储的开销。<code>SinkRecords</code> 而本质上和 <code>SourceRecords</code> 有着相同的信息: Kafka topic, partition, offset and the event key and value。</p>

    <p><code>flush()</code> 方法用于提交 offset 数据，这使得 task 可以从故障中恢复并且从安全点恢复，因此不会有事件丢失。该方法应该将任何未完成的数据推送到目标系统，然后阻塞，直到写入被确认。 参数 <code>offsets</code> 通常会被忽略，但在某些情况下很有用，例如实现需要在目标存储区中存储 offset 信息以提供精确的 exactly-once 交付。例如，HDFS connector 可以做到这一点，它使用原子性移动操作来确保 <code>flush()</code> 操作以原子方式将数据和 offset 提交到 HDFS 中的最终位置。</p>


    <h5><a id="connect_resuming" href="#connect_resuming">Resuming from Previous Offsets</a></h5>

    <p><code>SourceTask</code> 的实现包含每条记录的 stream ID (本示例中是输入的文件名) 和 offset (记录在文件中的位置)。该框架使用它定期提交 offset 数据，以便在发生故障的情况下，task 可以恢复并且最小化 reprocess 和 可能重复的事件的数量（或者从最近 Kafka Connect 正常停止过的 offset 恢复，例如在独立模式下或作业重新配置导致的停止）。这个提交过程由框架完全自动化，但只有 connector 知道如何退回到 input stream 中的正确位置并从该位置恢复。</p>

    <p>为了在启动时正确恢复，task 可以使用它的 <code>initialize()</code> 方法中传入的 <code>SourceContext</code> 来访问 offset 数据。在 <code>initialize()</code> 中，我们会添加更多的代码来读取 offset 数据（如果存在）并寻找对应的位置：</p>

    <pre class="brush: java;">
        stream = new FileInputStream(filename);
        Map&lt;String, Object&gt; offset = context.offsetStorageReader().offset(Collections.singletonMap(FILENAME_FIELD, filename));
        if (offset != null) {
            Long lastRecordedOffset = (Long) offset.get("position");
            if (lastRecordedOffset != null)
                seekToOffset(stream, lastRecordedOffset);
        }
    </pre>

    <p>当然，您可能需要为每个 input stream 读取多个 key。<code>OffsetStorageReader</code> 接口允许您发出批量读取以有效加载所有 offset，然后通过查找每个输入流来把它们放到对应的位置。</p>

    <h4><a id="connect_dynamicio" href="#connect_dynamicio">动态输入/输出流</a></h4>

    <p>Kafka Connect 旨在定义批量数据复制作业，例如复制整个数据库，而不是创建许多作业以分别复制每张表。这种设计的一个结果是，connector 的输入或输出流的集合可以随时间变化。</p>

    <p>Source connector 需要监视源系统的变化，例如数据库中表的增加与删除。当数据库中的表发生改变时，他们应通过<code>ConnectorContext</code>对象通知框架需要重新配置。例如，在 <code>SourceConnector</code> 中：</p>

    <pre class="brush: java;">
        if (inputsChanged())
            this.context.requestTaskReconfiguration();
    </pre>

    <p>该框架将及时请求新的配置信息并更新 task，使它们能够在再次配置之前正常地提交进度。请注意，在 <code>SourceConnector</code> 中，该监视功能当前由 connector 实现决定。如果需要额外的线程来执行监视，则连接器必须自行分配线程。</p>

    <p>理想情况下，用于监视变动的的代码将独立于 <code>Connector</code>，不会对 task 有任何影响。然而，变动也可以影响 task，最常见的是其输入流之一在输入系统中被破坏，例如，如果一个表从数据库中删除。如果<code>Task</code>在<code>Connector</code>之前遇到问题，这在<code>Connector</code>轮询数据源变动的时候很常见，需要由 <code>Task</code> 处理后续的 error。谢天谢地，这通常可以通过捕捉和处理适当的 exception 来处理。</p>

    <p><code>SinkConnectors</code> 通常只需要处理 stream 中增加的内容，这可能会转化为其输出中的新条目（例如新的数据库表）。该框架负责管理 Kafka 输入发生的任何更改，例如，由于批量订阅而导致输入主题集发生更改时。 <code>SinkTasks</code> 应该期望新的输入流，这可能需要在下游系统中创建新资源，例如数据库中的新表。在这些情况下最棘手的情况可能是多个 <code>SinkTasks</code> 第一次看到新的输入流并同时尝试创建新的资源。另一方面，<code>SinkConnectors</code>通常不需要特殊的代码来处理动态的流集。</p>

    <h4><a id="connect_configs" href="#connect_configs">连接配置验证</a></h4>

    <p>Kafka Connect 允许在提交要执行的 connector 之前验证 connector 配置，并且可以提供有关错误和建议值的反馈。为了利用这一点，连接器开发人员需要提供一个<code>config()</code>的实现来将配置定义公开给框架。</p>

    <p>下面 <code>FileStreamSourceConnector</code> 中的代码定义了配置并将其公开给框架。</p>

    <pre class="brush: java;">
        private static final ConfigDef CONFIG_DEF = new ConfigDef()
            .define(FILE_CONFIG, Type.STRING, Importance.HIGH, "Source filename.")
            .define(TOPIC_CONFIG, Type.STRING, Importance.HIGH, "The topic to publish data to");

        public ConfigDef config() {
            return CONFIG_DEF;
        }
    </pre>

    <p><code>ConfigDef</code> 类用于指定一组预期的配置。对于每个配置，您可以指定名称，类型，默认值，文档，组信息，组中的顺序，配置值的宽度以及适合在UI中显示的名称。另外，您可以通过覆盖 <code>Validator</code> 类来提供用于单个配置验证的特殊验证逻辑。此外，由于配置之间可能存在依赖关系，例如，配置的有效值和可见性可能会根据配置中的其它值而改变。为了解决这个问题，<code>ConfigDef</code> 允许你指定配置的依赖关系，并提供了一个 <code>Recommender</code> 的实现来获取有效的值，并给定当前配置值的配置可见性。</p>

    <p>另外，<code>Connector</code> 中的 <code>validate()</code> 方法提供了一个默认验证的实现，该实现返回允许配置的列表以及每个配置的配置错误和推荐值。但是，它不使用建议的值进行配置验证。您可以覆盖默认实现以提供自定义的配置验证，该自定义配置验证可能使用推荐值。</p>

    <h4><a id="connect_schemas" href="#connect_schemas">处理模式</a></h4>

    <p>FileStream connector 是个很好的例子，因为它们很简单，但它们也有着简单的结构化数据——每行只是一个字符串。几乎所有实用的 connector 都需要具有更复杂数据格式的模式。</p>

    <p>要创建更复杂的数据，您需要使用 Kafka Connect 的<code>data</code> API。大多数结构化记录除了原始类型外还需要与两个类进行交互：<code>Schema</code> 和 <code>Struct</code>。</p>

    <p>其 API 文档提供了一个完整的参考，但下面是一个创建 <code>Schema</code> 和 <code>Struct</code> 的简单示例：:</p>

    <pre class="brush: java;">
    Schema schema = SchemaBuilder.struct().name(NAME)
        .field("name", Schema.STRING_SCHEMA)
        .field("age", Schema.INT_SCHEMA)
        .field("admin", new SchemaBuilder.boolean().defaultValue(false).build())
        .build();

    Struct struct = new Struct(schema)
        .put("name", "Barbara Liskov")
        .put("age", 75);
    </pre>

    <p>如果您正在实现一个 source connector，则需要确定何时以及如何创建模式。在可能的情况下，应尽可能避免重新计算它们。例如，如果您的 connector 确定有固定模式，请静态创建并重用单个实例。</p>

    <p>但是，许多 connector 都有动态模式。一个简单的例子就是 database connector。考虑到即使数据库只有一个表，也不会为整个 connector 预定义模式（因为它随表格而变化）。但是在 connector 的整个生命周期中，它也可能不会因为只有一张表而使用固定模式，因为用户可能会执行 <code>ALTER TABLE</code> 命令，connector必须能够检测到这些变化并做出适当的反应。</p>

    <p>Sink connectors 通常更简单，因为它们在消耗数据，因此不需要创建模式。但是，他们应该同样仔细地验证他们收到的模式是否具有预期的格式。当模式不匹配时，通常表明上游 produce 正在生成无法正确转换到目标系统的无效数据，sink connector 应该抛出异常来向系统指示此错误。</p>

    <h4><a id="connect_administration" href="#connect_administration">Kafka Connect Administration</a></h4>

    <p>
        Kafka Connect 的 <a href="#connect_rest">REST layer</a> 层提供了一组 API 对群集进行管理。这包括查看 connector 配置和任务状态的 API，以及更改其当前行为的 API（例如更改配置和重新启动任务）。
    </p>

    <p>
        当 connector 首次提交到群集时，worker 将重新平衡群集中的全部 connector 及其 task，以便每个 worker的工作量大致相同。当 connector 增加或减少它们需要的任务数量或连接器的配置发生变化时，也使用相同的重新平衡过程。您可以使用 REST API 来查看 connector 及其 task 的当前状态，包括每个 connector 分配的 worker 的 ID。例如，查询文件源的状态（使用 GET/connectors/file-source/status）可能会产生如下输出：
    </p>

    <pre class="brush: json;">
    {
    "name": "file-source",
    "connector": {
        "state": "RUNNING",
        "worker_id": "192.168.1.208:8083"
    },
    "tasks": [
        {
        "id": 0,
        "state": "RUNNING",
        "worker_id": "192.168.1.209:8083"
        }
    ]
    }
    </pre>

    <p>
        Connector及其 task 将状态更新发布到群集中所有 worker 监视的共享主题（使用status.storage.topic配置）。由于 worker 异步使用此主题，因此状态更改通过状态 API 可见之前通常会有（短）延迟。以下状态可用于 connector 或其 task 之一：
    </p>

    <ul>
        <li><b>UNASSIGNED:</b> Connector/Task 还没有分配给一个worker。</li>
        <li><b>RUNNING:</b> Connector/Task 正在运行。</li>
        <li><b>PAUSED:</b> Connector/Task 已经被管理暂停。</li>
        <li><b>FAILED:</b> Connector/Task 失败了(通常是通过引发一个异常，在状态输出中报告)。</li>
    </ul>

    <p>
        在大多数情况下，connector 和 task 状态都会匹配，但在发生变动或 task 失败时，它们在短时间内可能会有所不同。例如，首次启动 connector 时，connector 及其 task 全部转换为 RUNNING 状态之前可能会有明显的延迟。由于 Connect 不会自动重启失败的 task，因此 task 失败时状态也会发生变化。要手动重新启动 connector/task，可以使用上面列出的重启 API。请注意，如果尝试在重新平衡过程中重启 task，Connect 将返回409（冲突）状态码。您可以在重新平衡完成后重试，但可能没有必要，因为重新平衡实际上会重启群集中的所有 connector 和 task。
    </p>

    <p>
        有时候暂时停止 connector 的消息处理很有用。例如，如果远程系统正在进行维护，则 source connector 最好停止从该系统中轮询获取新数据，而不是使用异常垃圾消息填充日志。对于这个用例，Connect 提供了一个 pause/resume API。当 source connector 暂停时，Connect 将停止轮询它以获取其他记录。当 sink connector 暂停时，Connect 将停止向其发送新消息。暂停状态是持久化的，因此即使重新启动群集，connector 也不会再次开始消息处理，直到 task 恢复。请注意，在所有 connector 的 task 都转换到 PAUSED 状态之前可能会有一段延迟，因为它们可能需要一些时间才能完成暂停过程中的任何处理。另外，失败的任务在重启之前不会转换到 PAUSED 状态。
    </p>
</script>

<div class="p-connect"></div>
