<!--
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->

<!-- should always link the the latest release's documentation -->
<!--
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->
<script>
  /*
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Define variables for doc templates
var context={
    "version": "10",
    "dotVersion": "1.0",
    "fullDotVersion": "1.0.0",
    "scalaVersion": "2.11"
};

<!--#include virtual="../js/templateData.js" --></script>

<script id="content-template" type="text/x-handlebars-template">
    <h1>编写自己的流处理程序</h1>
    <div class="sub-nav-sticky">
        <div class="sticky-top">
            <div style="height:35px">
                <a href="/{{version}}/documentation/streams/">简介</a>
                <a href="/{{version}}/documentation/streams/developer-guide.html">开发者指南</a>
                <a href="/{{version}}/documentation/streams/core-concepts.html">核心思想</a>
                <a  href="/{{version}}/documentation/streams/quickstart.html">运行 demo 程序</a>
                <a class="active-menu-item" href="/{{version}}/documentation/streams/tutorial.html">编写自己的流处理程序</a>
            </div>
        </div>
    </div>
    <p>
        在本指南中，我们将从头开始建立属于自己的项目，使用Kafka Streams编写一个流处理应用程序。如果你还没阅读过 <a href="/{{version}}/documentation/streams/quickstart">quickstart</a> (在Kafka流中运行一个流应用程序)章节，我们强烈建议你先去阅读一下。
    </p>

    <h4><a id="tutorial_maven_setup" href="#tutorial_maven_setup">建立一个Maven项目</a></h4>

    <p>
        使用以下命令来创建具有Kafka Streams项目架构的Maven原型:
    </p>

    <pre class="brush: bash;">
        mvn archetype:generate \
            -DarchetypeGroupId=org.apache.kafka \
            -DarchetypeArtifactId=streams-quickstart-java \
            -DarchetypeVersion={{fullDotVersion}} \
            -DgroupId=streams.examples \
            -DartifactId=streams.examples \
            -Dversion=0.1 \
            -Dpackage=myapps
    </pre>

    <p>
        对于 <code>groupId</code>， <code>artifactId</code> 和 <code>package</code> 这三个参数你可以设置不同的值。
        假定使用上述的参数值，该命令将创建如下的项目结构:
    </p>

    <pre class="brush: bash;">
        &gt; tree streams.examples
        streams-quickstart
        |-- pom.xml
        |-- src
            |-- main
                |-- java
                |   |-- myapps
                |       |-- LineSplit.java
                |       |-- Pipe.java
                |       |-- WordCount.java
                |-- resources
                    |-- log4j.properties
    </pre>

    <p>
        项目中包含的 <code>pom.xml</code> 已经定义了 Streams 依赖，并且在 <code>src/main/java</code> 下已经有几个用 Streams 库编写的示例程序。既然我们要从头开始编写这些程序，那我们现在可以删除这些示例:
    </p>

    <pre class="brush: bash;">
        &gt; cd streams-quickstart
        &gt; rm src/main/java/myapps/*.java
    </pre>

    <h4><a id="tutorial_code_pipe" href="#tutorial_code_pipe">编写第一个 Streams 应用: Pipe</a></h4>

    可以打开你喜欢的 IDE 并导入这个 Maven 项目，或者简单地打开一个文本编辑器，并在 <code>src/main/java</code> 下创建一个java文件。让我们命名为 <code>Pipe.java</code> :

    <pre class="brush: java;">
        package myapps;

        public class Pipe {

            public static void main(String[] args) throws Exception {

            }
        }
    </pre>

    <p>
        我们将在 <code>main</code> 函数中来编写这个 pipe 程序。注意，由于 ide 通常可以自动添加导入语句(import)，所以我们不会列出这些 import。但是，如果你使用的是文本编辑器，则需要手动添加导入，在本节的末尾，我们将向你展示带有 import 语句的完整代码片段。
    </p>

    <p>
        写 Streams 应用程序的第一步是创建一个 <code>java.util.Properties</code> 映射来保存定义在 <code>StreamsConfig</code> 中不同的 Streams 执行配置参数。几个重要的配置值需要设置: <code>StreamsConfig.BOOTSTRAP_SERVERS_CONFIG</code>,它指定用于和 Kafka 集群建立初始化连接的 host/port 对的列表, <code>StreamsConfig.APPLICATION_ID_CONFIG</code>,它提供了 Streams 应用程序的唯一标识符，以便于在同一个Kafka集群通信过程中区分自己和其他应用程序：
    </p>

    <pre class="brush: java;">
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-pipe");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");    // assuming that the Kafka broker this application is talking to runs on local machine with port 9092
    </pre>

    <p>
        另外，你可以在同一个映射中自定义其他配置，例如配置默认的序列化和反序列化库的键值对:
    </p>

    <pre class="brush: java;">
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
    </pre>

    <p>
        有关 Kafka Streams 配置的完整列表，请参阅此 <a href="/{{version}}/documentation/#streamsconfigs">table</a>。
    </p>

    <p>
        接下来我们将定义 Streams 应用程序的计算逻辑。在 Kafka Streams 中，这个计算逻辑被定义为<code>拓扑结构（topology）</code>（即连接起来的处理器节点（ processor nodes ）） 。 我们可以使用 topology builder 来构建这样的 topology，
    </p>

    <pre class="brush: java;">
        final StreamsBuilder builder = new StreamsBuilder();
    </pre>

    <p>
        然后使用这个 topology builder 从 <code>streams-plaintext-input</code> 这个 Kafka topic 中创建 source stream：
    </p>

    <pre class="brush: java;">
        KStream&lt;String, String&gt; source = builder.stream("streams-plaintext-input");
    </pre>

    <p>
        现在我们得到一个 <code>KStream</code>，它不断地从 <code>streams-plaintext-input</code> 这个 Kafka topic 的数据流源头中生成记录。
        记录被组织为 <code>String</code> 类型的 key-value 键值对。
        对于这个 stream，最简单处理的方法就是将它写入另一个 Kafka topic，比如这个名为 <code>streams-pipe-output</code> 的 topic :
    </p>

    <pre class="brush: java;">
        source.to("streams-pipe-output");
    </pre>

    <p>
        注意，我们也可以将上面的两行代码连接成一行:
    </p>

    <pre class="brush: java;">
        builder.stream("streams-plaintext-input").to("streams-pipe-output");
    </pre>

    <p>
        我们可以通过执行以下操作来检查此 builder 创建的 <code>topology</code> 结构类型：
    </p>

    <pre class="brush: java;">
        final Topology topology = builder.build();
    </pre>

    <p>
        并将其描述打印为标准输出:
    </p>

    <pre class="brush: java;">
        System.out.println(topology.describe());
    </pre>

    <p>
        如果我们只编码到这里，然后编译并运行此程序，它将输出以下信息:
    </p>

    <pre class="brush: bash;">
        &gt; mvn clean package
        &gt; mvn exec:java -Dexec.mainClass=myapps.Pipe
        Sub-topologies:
          Sub-topology: 0
            Source: KSTREAM-SOURCE-0000000000(topics: streams-plaintext-input) --> KSTREAM-SINK-0000000001
            Sink: KSTREAM-SINK-0000000001(topic: streams-pipe-output) <-- KSTREAM-SOURCE-0000000000
        Global Stores:
          none
    </pre>

    <p>
        如上所示，搭建好的拓扑结构有两个处理器节点（processor nodes），一个 source node <code>KSTREAM-SOURCE-0000000000</code> 和一个 sink node <code>KSTREAM-SINK-0000000001</code> 。
        <code>KSTREAM-SOURCE-0000000000</code> 持续地从 Kafka topic <code>streams-plaintext-input</code> 读取数据并将数据传递给下游节点 <code>KSTREAM-SINK-0000000001</code>；
        <code>KSTREAM-SINK-0000000001</code> 会将它接收到的数据按顺序写入到其他 Kafka topic <code>streams-pipe-output</code>
        （<code>--&gt;</code> 和 <code>&lt;--</code> 两个箭头分别指示该节点的下游和上游处理器节点 ，即拓扑结构中的父节点和子节点）。
        另外，这种简单的拓扑没有与之相关联的全局状态存储state-store（我们将在后面的章节中更多地讨论状态存储）。
    </p>

    <p>
        请注意，如上所述，我们在代码中创建拓扑结构的时候可以在任何给定点上声明拓扑结构，因此作为用户，您可以交互式地“尝试并品尝”拓扑中定义的计算逻辑，直到您满意为止。
        假设我们已经完成了这个简单的拓扑结构，它只是以一种无尽的流处理方式将数据从一个Kafka topic 传递到另一个 topic，
        我们现在可以使用我们刚刚构建的两个组件构建Streams客户端：配置映射和拓扑对象（也可以从<code> props </code>映射构造一个<code> StreamsConfig </code>对象，然后将该对象传递给构造函数，
        <code> KafkaStreams </code>具有重载的构造函数以接受任意类型）。
    </p>

    <pre class="brush: java;">
        final KafkaStreams streams = new KafkaStreams(topology, props);
    </pre>

    <p>
        通过调用它的<code> start()</code>函数，我们可以触发这个客户端开始运行。
        除非在客户端上调用<code> close()</code>函数，否则程序不会停止。
        例如，我们可以添加一个带有倒计数锁存器的关闭钩子来捕获用户中断，并在终止该程序时关闭客户端：
    </p>

    <pre class="brush: java;">
        final CountDownLatch latch = new CountDownLatch(1);

        // attach shutdown handler to catch control-c
        Runtime.getRuntime().addShutdownHook(new Thread("streams-shutdown-hook") {
            @Override
            public void run() {
                streams.close();
                latch.countDown();
            }
        });

        try {
            streams.start();
            latch.await();
        } catch (Throwable e) {
            System.exit(1);
        }
        System.exit(0);
    </pre>

    <p>
        到目前为止完整的代码如下所示：
    </p>

    <pre class="brush: java;">
        package myapps;

        import org.apache.kafka.common.serialization.Serdes;
        import org.apache.kafka.streams.KafkaStreams;
        import org.apache.kafka.streams.StreamsBuilder;
        import org.apache.kafka.streams.StreamsConfig;
        import org.apache.kafka.streams.Topology;

        import java.util.Properties;
        import java.util.concurrent.CountDownLatch;

        public class Pipe {

            public static void main(String[] args) throws Exception {
                Properties props = new Properties();
                props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-pipe");
                props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
                props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
                props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

                final StreamsBuilder builder = new StreamsBuilder();

                builder.stream("streams-plaintext-input").to("streams-pipe-output");

                final Topology topology = builder.build();

                final KafkaStreams streams = new KafkaStreams(topology, props);
                final CountDownLatch latch = new CountDownLatch(1);

                // attach shutdown handler to catch control-c
                Runtime.getRuntime().addShutdownHook(new Thread("streams-shutdown-hook") {
                    @Override
                    public void run() {
                        streams.close();
                        latch.countDown();
                    }
                });

                try {
                    streams.start();
                    latch.await();
                } catch (Throwable e) {
                    System.exit(1);
                }
                System.exit(0);
            }
        }
    </pre>

    <p>
        如果你已经有一个正在运行的 Kafka broker （服务地址是：<code>localhost:9092</code>），
        并且 <code>streams-plaintext-input</code> 和 <code>streams-pipe-output</code> 这两个 topic 也都创建好了，
        那么就可以在IDE或者命令行上运行程序，使用如下 Maven 命令：
    </p>

    <pre class="brush: brush;">
        &gt; mvn clean package
        &gt; mvn exec:java -Dexec.mainClass=myapps.Pipe
    </pre>

    <p>
        有关如何运行Streams应用程序并观察计算结果的详细说明，请阅读<a href="/{{version}}/documentation/streams/quickstart">运行 demo 程序</a>部分。
        本节的其余部分我们不会谈论这一点。
    </p>

    <h4><a id="tutorial_code_linesplit" href="#tutorial_code_linesplit">编写第二个流处理程序：Line Split</a></h4>

    <p>
        我们已经学会了如何使用 kafka Streams 的两个关键组件：<code> StreamsConfig </code>和<code> 拓扑结构（Topology） </code>来构建Streams客户端。
        现在让我们继续给当前的拓扑结构添加一些实际的处理逻辑。
        我们可以首先复制现有的<code> Pipe.java </code>类来创建另一个程序：
    </p>

    <pre class="brush: brush;">
        &gt; cp src/main/java/myapps/Pipe.java src/main/java/myapps/LineSplit.java
    </pre>

    <p>
        并更改其类名以及应用程序ID配置以与原始程序区分开来：
    </p>

    <pre class="brush: java;">
        public class LineSplit {

            public static void main(String[] args) throws Exception {
                Properties props = new Properties();
                props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-linesplit");
                // ...
            }
        }
    </pre>

    <p>
        由于每个 Stream 提供的记录都是一个<code> String </code>类型的键值对，因此我们将 value 值视为一行文本，并使用<code> FlatMapValues </code>运算符将其拆分为单词：
    </p>

    <pre class="brush: java;">
        KStream&lt;String, String&gt; source = builder.stream("streams-plaintext-input");
        KStream&lt;String, String&gt; words = source.flatMapValues(new ValueMapper&lt;String, Iterable&lt;String&gt;&gt;() {
                    @Override
                    public Iterable&lt;String&gt; apply(String value) {
                        return Arrays.asList(value.split("\\W+"));
                    }
                });
    </pre>

    <p>

        运算符将<code> source </code>流作为其输入，按顺序处理源流中的每条记录，并通过将记录的value值分解为一个字符串列表，以此来生成一个名为<code> words </code>的新的流，
        并将每个单词作为输出流<code> words </code>的新记录。
        这是一个无状态的操作符，无需跟踪之前收到的记录或处理结果。
        请注意，如果您使用的是JDK 8，则可以使用lambda表达式并简化上面的代码：
    </p>

    <pre class="brush: java;">
        KStream&lt;String, String&gt; source = builder.stream("streams-plaintext-input");
        KStream&lt;String, String&gt; words = source.flatMapValues(value -> Arrays.asList(value.split("\\W+")));
    </pre>

    <p>
        最后，我们可以将 word 流写回另一个Kafka topic，比如<code> streams-linesplit-output </code>。
        再次，这两个步骤可以如下所示连接（假设使用lambda表达式）：
    </p>

    <pre class="brush: java;">
        KStream&lt;String, String&gt; source = builder.stream("streams-plaintext-input");
        source.flatMapValues(value -> Arrays.asList(value.split("\\W+")))
              .to("streams-linesplit-output");
    </pre>

    <p>
        如果我们现在将这个扩展拓扑描述为<code> System.out.println(topology.describe())</code>，我们将得到以下结果：
    </p>

    <pre class="brush: bash;">
        &gt; mvn clean package
        &gt; mvn exec:java -Dexec.mainClass=myapps.LineSplit
        Sub-topologies:
          Sub-topology: 0
            Source: KSTREAM-SOURCE-0000000000(topics: streams-plaintext-input) --> KSTREAM-FLATMAPVALUES-0000000001
            Processor: KSTREAM-FLATMAPVALUES-0000000001(stores: []) --> KSTREAM-SINK-0000000002 <-- KSTREAM-SOURCE-0000000000
            Sink: KSTREAM-SINK-0000000002(topic: streams-linesplit-output) <-- KSTREAM-FLATMAPVALUES-0000000001
          Global Stores:
            none
    </pre>

    <p>
        正如我们上面看到的那样，一个新的处理器节点<code> KSTREAM-FLATMAPVALUES-0000000001 </code>被注入到原始 source node 和 sink node 之间的拓扑结构中。
        它将 source node 作为其父节点并将 sink node 作为其子节点。
        换句话说，从 source node 获取的每个记录将首先传递给新加入的<code> KSTREAM-FLATMAPVALUES-0000000001 </code>节点进行处理，然后生成一个或多个新记录作为输出结果。
        这些新记录会持续传递给 sink node 并最终回写到 kafka 。
        注意这个处理器节点是“无状态的”，因为它不与任何 stores 相关联（即<code>(stores: [])</code>）。
    </p>

    <p>
        完整的代码如下所示（假设使用lambda表达式）：
    </p>

    <pre class="brush: java;">
        package myapps;

        import org.apache.kafka.common.serialization.Serdes;
        import org.apache.kafka.streams.KafkaStreams;
        import org.apache.kafka.streams.StreamsBuilder;
        import org.apache.kafka.streams.StreamsConfig;
        import org.apache.kafka.streams.Topology;
        import org.apache.kafka.streams.kstream.KStream;

        import java.util.Arrays;
        import java.util.Properties;
        import java.util.concurrent.CountDownLatch;

        public class LineSplit {

            public static void main(String[] args) throws Exception {
                Properties props = new Properties();
                props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-linesplit");
                props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
                props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
                props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

                final StreamsBuilder builder = new StreamsBuilder();

                KStream&lt;String, String&gt; source = builder.stream("streams-plaintext-input");
                source.flatMapValues(value -> Arrays.asList(value.split("\\W+")))
                      .to("streams-linesplit-output");

                final Topology topology = builder.build();
                final KafkaStreams streams = new KafkaStreams(topology, props);
                final CountDownLatch latch = new CountDownLatch(1);

                // ... same as Pipe.java above
            }
        }
    </pre>

    <h4><a id="tutorial_code_wordcount" href="#tutorial_code_wordcount">编写第三个流处理程序： Wordcount</a></h4>

    <p>
        现在让我们进一步添加一些“有状态”计算过程（计算从源文本流中拆分出来的单词出现的次数）到拓扑结构中。
        按照类似的步骤，我们创建另一个基于<code> LineSplit.java </code>类的程序：
    </p>

    <pre class="brush: java;">
        public class WordCount {

            public static void main(String[] args) throws Exception {
                Properties props = new Properties();
                props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-wordcount");
                // ...
            }
        }
    </pre>

    <p>
        为了计算单词，我们可以首先修改<code> flatMapValues </code>运算符，将它们全部转为小写字母（假设使用lambda表达式）：
    </p>

    <pre class="brush: java;">
        source.flatMapValues(new ValueMapper&lt;String, Iterable&lt;String&gt;&gt;() {
                    @Override
                    public Iterable&lt;String&gt; apply(String value) {
                        return Arrays.asList(value.toLowerCase(Locale.getDefault()).split("\\W+"));
                    }
                });
    </pre>

    <p>
        为了进行计数聚合，我们必须首先指定我们要使用<code> groupBy </code>运算符将 value 字符串（即小写单词）输入到流中。
        该运算符生成一个新的分组流，然后可以通过<code> count </code>运算符进行聚合，该运算符会在每个分组键上持续计数：
    </p>

    <pre class="brush: java;">
        KTable&lt;String, Long&gt; counts =
        source.flatMapValues(new ValueMapper&lt;String, Iterable&lt;String&gt;&gt;() {
                    @Override
                    public Iterable&lt;String&gt; apply(String value) {
                        return Arrays.asList(value.toLowerCase(Locale.getDefault()).split("\\W+"));
                    }
                })
              .groupBy(new KeyValueMapper&lt;String, String, String&gt;() {
                   @Override
                   public String apply(String key, String value) {
                       return value;
                   }
                })
              // Materialize the result into a KeyValueStore named "counts-store".
              // The Materialized store is always of type &lt;Bytes, byte[]&gt; as this is the format of the inner most store.
              .count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, byte[]&gt;&gt; as("counts-store"));
    </pre>

    <p>
        请注意，<code> count </code>运算符具有<code> Materialized </code>参数，该参数指定运行中的计数应存储在名为<code> counts-store </code>的 state store 中。
        这个<code> Counts store </code> 支持实时查询，在<a href="/{{version}}/documentation/streams/developer-guide#streams_interactive_queries">开发者指南</a>中有更详细的描述。
    </p>

    <p>
        我们还可以将<code> counts </code> KTable 的 changelog Stream 写回到另一个 Kafka topic 中，比如<code> streams-wordcount-output </code>。
        因为结果是一个 changlog stream ，所以 output topic <code> streams-wordcount-output </code> 需要配置为启用日志压缩。
        请注意，这次的值类型不再是<code> String类型 </code>而是<code> Long类型 </code>，所以默认的序列化类已经不适合将long类型的结果写回 Kafka 。
        我们需要为<code> Long </code>类型提供重写之后的序列化方法，否则将抛出运行时异常：
    </p>

    <pre class="brush: java;">
        counts.toStream().to("streams-wordcount-output", Produced.with(Serdes.String(), Serdes.Long());
    </pre>

    <p>
        请注意，为了从 <code> streams-wordcount-output </code> 这个 topic 读取 changelog stream ，
        需要将值反序列化设置为<code> org.apache.kafka.common.serialization.LongDeserializer </code>。
        有关详细信息，请参见<a href="/{{version}}/documentation/streams/quickstart">运行 demo 程序</a>一节。
        假设可以使用来自JDK 8的lambda表达式，上面的代码可以简化为：
    </p>

    <pre class="brush: java;">
        KStream&lt;String, String&gt; source = builder.stream("streams-plaintext-input");
        source.flatMapValues(value -> Arrays.asList(value.toLowerCase(Locale.getDefault()).split("\\W+")))
              .groupBy((key, value) -> value)
              .count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, byte[]&gt;&gt;as("counts-store"))
              .toStream()
              .to("streams-wordcount-output", Produced.with(Serdes.String(), Serdes.Long());
    </pre>

    <p>
        如果我们再次将此扩展拓扑描述为<code> System.out.println(topology.describe())</code>，我们将得到以下结果：
    </p>

    <pre class="brush: bash;">
        &gt; mvn clean package
        &gt; mvn exec:java -Dexec.mainClass=myapps.WordCount
        Sub-topologies:
          Sub-topology: 0
            Source: KSTREAM-SOURCE-0000000000(topics: streams-plaintext-input) --> KSTREAM-FLATMAPVALUES-0000000001
            Processor: KSTREAM-FLATMAPVALUES-0000000001(stores: []) --> KSTREAM-KEY-SELECT-0000000002 <-- KSTREAM-SOURCE-0000000000
            Processor: KSTREAM-KEY-SELECT-0000000002(stores: []) --> KSTREAM-FILTER-0000000005 <-- KSTREAM-FLATMAPVALUES-0000000001
            Processor: KSTREAM-FILTER-0000000005(stores: []) --> KSTREAM-SINK-0000000004 <-- KSTREAM-KEY-SELECT-0000000002
            Sink: KSTREAM-SINK-0000000004(topic: Counts-repartition) <-- KSTREAM-FILTER-0000000005
          Sub-topology: 1
            Source: KSTREAM-SOURCE-0000000006(topics: Counts-repartition) --> KSTREAM-AGGREGATE-0000000003
            Processor: KSTREAM-AGGREGATE-0000000003(stores: [Counts]) --> KTABLE-TOSTREAM-0000000007 <-- KSTREAM-SOURCE-0000000006
            Processor: KTABLE-TOSTREAM-0000000007(stores: []) --> KSTREAM-SINK-0000000008 <-- KSTREAM-AGGREGATE-0000000003
            Sink: KSTREAM-SINK-0000000008(topic: streams-wordcount-output) <-- KTABLE-TOSTREAM-0000000007
        Global Stores:
          none
    </pre>

    <p>
        如上所述，拓扑现在包含两个断开的子拓扑。
        第一个子拓扑的 sink node <code>KSTREAM-SINK-0000000004 </code>将写入一个重新分区的 topic ——<code> Counts-repartition </code>，
        并将由第二个子拓扑的 source node<code> KSTREAM-SOURCE-0000000006 </code>读取。
        重分区的 Topic 是通过聚合键（本例中的聚合键为值字符串）“混洗”源流。
        此外，在第一个子拓扑结构中，为了取出聚合键为空的中间记录，一个无状态的<code> KSTREAM-FILTER-0000000005 </code>被嵌入到在分组节点<code> KSTREAM-KEY-SELECT-0000000002 </code>节点和 sink node 之间。
    </p>
    <p>
        在第二个子拓扑中，聚合节点<code> KSTREAM-AGGREGATE-0000000003 </code>与名为<code> Counts </code>的 state store 结合在一起（state store 的名称由用户在<code> cout</code>运算符指定）。
        从 source node 接收记录时，聚合处理器将首先查询其关联的<code> Counts </code>store ，以获得该key值当前的计数，增加1，然后将新计数写回到 store。
        每个更新的密钥计数都将传送到<code> KTABLE-TOSTREAM-0000000007 </code>节点的下游，该节点将count的更新流解析为记录流，然后再进一步传输到 sink node <code> KSTREAM-SINK-0000000008 </code> 以便写回 kafka 系统。
    </p>

    <p>
        完整的代码如下所示（假设使用lambda表达式）：
    </p>

    <pre class="brush: java;">
        package myapps;

        import org.apache.kafka.common.serialization.Serdes;
        import org.apache.kafka.streams.KafkaStreams;
        import org.apache.kafka.streams.StreamsBuilder;
        import org.apache.kafka.streams.StreamsConfig;
        import org.apache.kafka.streams.Topology;
        import org.apache.kafka.streams.kstream.KStream;

        import java.util.Arrays;
        import java.util.Locale;
        import java.util.Properties;
        import java.util.concurrent.CountDownLatch;

        public class WordCount {

            public static void main(String[] args) throws Exception {
                Properties props = new Properties();
                props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-wordcount");
                props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
                props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
                props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

                final StreamsBuilder builder = new StreamsBuilder();

                KStream&lt;String, String&gt; source = builder.stream("streams-plaintext-input");
                source.flatMapValues(value -> Arrays.asList(value.toLowerCase(Locale.getDefault()).split("\\W+")))
                      .groupBy((key, value) -> value)
                      .count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, byte[]&gt;&gt;as("counts-store"))
                      .toStream()
                      .to("streams-wordcount-output", Produced.with(Serdes.String(), Serdes.Long());

                final Topology topology = builder.build();
                final KafkaStreams streams = new KafkaStreams(topology, props);
                final CountDownLatch latch = new CountDownLatch(1);

                // ... same as Pipe.java above
            }
        }
    </pre>

    <div class="pagination">
        <a href="/{{version}}/documentation/streams/quickstart" class="pagination__btn pagination__btn__prev">Previous</a>
        <a href="/{{version}}/documentation/streams/developer-guide" class="pagination__btn pagination__btn__next">Next</a>
    </div>
</script>

<div class="p-quickstart-streams"></div>

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html xmlns:og="http://ogp.me/ns#">

<head>
	<title>Kafka 中文文档 - ApacheCN</title>
	<link rel='stylesheet' href='./css/styles.css' type='text/css'>
	<link rel='stylesheet' href='./css/syntax-highlighting.css' type='text/css'>
	<link rel="icon" type="image/gif" href="./images/apache_feather.gif">
	<meta name="robots" content="index,follow" />
	<meta name="language" content="en" />
	<meta name="keywords" content="apache kafka messaging queuing distributed stream processing">
	<meta name="description" content="Apache Kafka: A Distributed Streaming Platform.">
	<meta http-equiv='Content-Type' content='text/html;charset=utf-8' />
	<meta name="viewport" content="initial-scale = 1.0,maximum-scale = 1.0" />
	<meta property="og:title" content="Apache Kafka" />
	<meta property="og:image" content="http://apache-kafka.org/images/apache-kafka.png" />
	<meta property="og:description" content="Apache Kafka: A Distributed Streaming Platform." />
	<meta property="og:site_name" content="Apache Kafka" />
	<meta property="og:type" content="website" />
	<link href="https://fonts.googleapis.com/css?family=Cutive+Mono|Roboto:400,700,900" rel="stylesheet">
	<script src="./js/jquery.min.js"></script>
	<script async src="./js/apachecn/googletagmanager.js"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag() { dataLayer.push(arguments); }
		gtag('js', new Date());

		gtag('config', 'UA-102475051-9');
	</script>

	<script>
		var _hmt = _hmt || [];
		(function () {
			var hm = document.createElement("script");
			hm.src = "https://hm.baidu.com/hm.js?9f2b74b80ab8aafb5970835acf96a0ea";
			var s = document.getElementsByTagName("script")[0];
			s.parentNode.insertBefore(hm, s);
		})();
	</script>

	<script>
		(function () {
			var bp = document.createElement('script');
			var curProtocol = window.location.protocol.split(':')[0];
			if (curProtocol === 'https') {
				bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
			}
			else {
				bp.src = 'http://push.zhanzhang.baidu.com/push.js';
			}
			var s = document.getElementsByTagName("script")[0];
			s.parentNode.insertBefore(bp, s);
		})();
	</script>
	<script>
		// DO NOT NEED TO UPDATE
		// Legacy versions of the documentation to not do frontend redirect for
		// These docs are written as a single super long file so no need to re-route
		var legacyDocPaths = [
			'./07/documentation',
			'./07/documentation/',
			'./08/documentation',
			'./08/documentation/',
			'./081/documentation',
			'./081/documentation/',
			'./082/documentation',
			'./082/documentation/',
			'./090/documentation',
			'./090/documentation/',
			'./0100/documentation',
			'./0100/documentation/'
		];

		// Any direct request for Streams documentation in docs versions prior to 0101
		// Redirect these requests to the standalone Streams doc page
		var currentPath = window.location.pathname;
		var shouldRedirect = !legacyDocPaths.includes(currentPath);
		var isDocumenationPage = currentPath.includes('/documentation');

		var hasNotSpecifiedFullPath = !currentPath.includes('/documentation/streams') && !currentPath.includes('/documentation/streams/');

		// Look for legacy anchors to clue us in on what full path the user needs
		// Add more as needed
		var specifiedStreamsAnchor = window.location.hash.includes('#streams_');

		if (shouldRedirect && isDocumenationPage && hasNotSpecifiedFullPath) {
			if (specifiedStreamsAnchor) {
				window.location.pathname = currentPath + 'streams';
			}
		}
	</script>
</head>
<!--#include virtual="../../includes/_header.htm" -->
<body>
	<div class="main">
		<div class="header">
			<a href="/"><img width="325" height="97" class="logo" src="/images/logo.png"></a>
		</div>

<!--#include virtual="../../includes/_top.htm" -->
<div class="content documentation documentation--current">
        <nav class="b-sticky-nav">
  <div class="nav-scroller">
    <div class="nav__inner">
      <a class="b-nav__home nav__item" href="/">主页</a>
      <a class="b-nav__intro nav__item" href="/intro.html">介绍</a>
      <a class="b-nav__quickstart nav__item" href="/quickstart.html">快速开始</a>
      <a class="b-nav__uses nav__item" href="/uses.html">使用案例</a>

      <div class="nav__item nav__item__with__subs">
        <a class="b-nav__docs nav__item nav__sub__anchor" href="/documentation.html">文档</a>
        <a class="nav__item nav__sub__item" href="/documentation.html#gettingStarted">入门</a>
        <a class="nav__item nav__sub__item" href="/documentation.html#api">APIs</a>
        <a class="b-nav__streams nav__item nav__sub__item" href="/documentation.html#streams">kafka streams</a>
        <a class="nav__item nav__sub__item" href="/documentation.html#connect">kafka connect</a>
        <a class="nav__item nav__sub__item" href="/documentation.html#configuration">配置</a>
        <a class="nav__item nav__sub__item" href="/documentation.html#design">设计</a>
        <a class="nav__item nav__sub__item" href="/documentation.html#implementation">实现</a>
        <a class="nav__item nav__sub__item" href="/documentation.html#operations">操作</a>
        <a class="nav__item nav__sub__item" href="/documentation.html#security">安全</a>
      </div>

      <a class="b-nav__performance nav__item" href="/performance.html">性能</a>
      <a class="b-nav__poweredby nav__item" href="/powered-by.html">powered by</a>
      <a class="b-nav__project nav__item" href="/project.html">项目信息</a>
      <a class="b-nav__ecosystem nav__item" href="https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem" target="_blank">生态圈</a>
      <a class="b-nav__clients nav__item" href="https://cwiki.apache.org/confluence/display/KAFKA/Clients" target="_blank">客户端</a>
      <a class="b-nav__events nav__item" href="/events.html">事件</a>
      <a class="b-nav__contact nav__item" href="/contact.html">联系我们</a>

      <div class="nav__item nav__item__with__subs">
        <a class="b-nav__apache nav__item nav__sub__anchor b-nav__sub__anchor" href="#">apache</a>
        <a class="b-nav__apache nav__item nav__sub__item" href="http://www.apache.org/" target="_blank">贡献</a>
        <a class="b-nav__apache nav__item nav__sub__item" href="http://www.apache.org/licenses/" target="_blank">license</a>
        <a class="b-nav__apache nav__item nav__sub__item" href="http://www.apache.org/foundation/sponsorship.html" target="_blank">赞助</a>
        <a class="b-nav__apache nav__item nav__sub__item" href="http://www.apache.org/foundation/thanks.html" target="_blank">感谢</a>
        <a class="b-nav__apache nav__item nav__sub__item" href="http://www.apache.org/security/" target="_blank">安全</a>
      </div>

      <a class="btn" href="/downloads.html">下载</a>
      <div class="social-links">
        <a class="twitter" href="https://twitter.com/apachekafka" target="_blank">@apachekafka</a>
      </div>
    </div>
  </div>
  <div class="navindicator">
    <div class="b-nav__home navindicator__item"></div>
    <div class="b-nav__intro navindicator__item"></div>
    <div class="b-nav__quickstart navindicator__item"></div>
    <div class="b-nav__uses navindicator__item"></div>
    <div class="b-nav__docs navindicator__item"></div>
    <div class="b-nav__performance navindicator__item"></div>
    <div class="b-nav__poweredby navindicator__item"></div>
    <div class="b-nav__project navindicator__item"></div>
    <div class="b-nav__ecosystem navindicator__item"></div>
    <div class="b-nav__clients navindicator__item"></div>
    <div class="b-nav__events navindicator__item"></div>
    <div class="b-nav__contact navindicator__item"></div>
  </div>
</nav>

        <!--#include virtual="../../includes/_nav.htm" -->
        <div class="right">
                <a class="documentation__banner b-sticky-doc-banner" href="/documentation">
  You're viewing documentation for an older version of Kafka - check out our current documentation here.
</a>

            <!--#include virtual="../../includes/_docs_banner.htm" -->
        <ul class="breadcrumbs">
            <li><a href="/documentation">Documentation</a></li>
            <li><a href="/documentation/streams">Streams</a></li>
        </ul>
        <div class="p-content"></div>
    </div>
</div>
				</div>
			</div>
		</div>
		<div class="footer">
			<div class="footer__inner">
				<div class="footer__legal">
					<span class="footer__legal__one">The contents of this website are &copy; 2016 <a href="https://www.apache.org/" target="_blank">Apache Software Foundation</a> under the terms of the <a href="https://www.apache.org/licenses/LICENSE-2.0.html" target="_blank">Apache License v2</a>.</span>
					<span class="footer__legal__two">Apache Kafka, Kafka, and the Kafka logo are either registered trademarks or trademarks of The Apache Software Foundation</span>
					<span class="footer__legal__three">in the United States and other countries.</span>
				</div>
				<a class="apache-feather" target="_blank" href="http://www.apache.org">
					<img width="40" src="/images/feather-small.png" alt="Apache Feather">
				</a>
			</div>
		</div>
	</body>

    <script type="text/javascript" src="/js/syntaxhighlighter.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/2.0.0/handlebars.js"></script>
	<script>
		$(function () {
			// list of pages that are rendered with Handlebars
			var templates = [
				'introduction',
				'implementation',
				'design',
				'api',
				'configuration',
				'ops',
				'security',
				'connect',
				'streams',
				'quickstart',
				'toc',
				'upgrade',
				'content'
			];

			// loop through all Handlebar templates on the page and render them
			for(var i = 0; i < templates.length; i++) {
				var templateScript = $("#" + templates[i] + "-template").html();
				if(templateScript) {
					var template = Handlebars.compile(templateScript);
					var html = template(context);
					$(".p-" + templates[i]).html(html);
				}
			}
		});
	</script>

	<script src="/js/jquery.sticky-kit.min.js"></script>
	<script>
		$(function() {
			// Set mobile scroll position on nav
			function setNavScroll(offsetLeft) {
				$('.nav-scroller').animate({
					scrollLeft: $('.nav-scroller').scrollLeft() + $('nav .selected').offset().left - offsetLeft
				}, 50);
			}

			// Helper classes for nav
			$('nav').mouseenter(function(){
				$(this).addClass('hovering');
			});
			$('nav').mouseleave(function(){
				$(this).removeClass('hovering');
			});

			// Handle expanding sections of nav (async)
			$('.b-nav__sub__anchor').click(function(){
				$('nav .selected').removeClass('selected');
				$('.nav__item__with__subs--expanded').removeClass('nav__item__with__subs--expanded');

				$(this).addClass('selected');
				$(this).parent().toggleClass('nav__item__with__subs--expanded');

				if($(window).width() <= 650) {
					window.setTimeout(function(){
						setNavScroll(30);
					}, 300);
				}
			});

			// Initialize sticky elements on the page
			if($(window).width() > 650) {
				// Nav for desktop
				$('.b-sticky-nav').stick_in_parent({offset_top: 40});
				// Documentation banner for desktop
				$('.b-sticky-doc-banner').stick_in_parent({offset_top: 0});
			}	else {
				// Scroll nav for mobile so current nav item is in view
				window.setTimeout(function(){
					setNavScroll(80);
				}, 300);
			}

			// On window resize check to see if stuff should be unstuck
			window.onresize = function(event) {
			  if($(window).width() <= 650) {
			    $('.b-sticky-nav').trigger("sticky_kit:detach");
			  } else {
			    $('.b-sticky-nav').stick_in_parent({offset_top: 40});
					$('.b-sticky-doc-banner').stick_in_parent({offset_top: 0});
			  }
			};
		});
	</script>


</html>

<!--#include virtual="../../includes/_footer.htm" -->
<script>
$(function() {
          // Show selected style on nav item
          $('.b-nav__streams').addClass('selected');
  
          //sticky secondary nav
          var $navbar = $(".sub-nav-sticky"),
               y_pos = $navbar.offset().top,
               height = $navbar.height();
       
           $(window).scroll(function() {
               var scrollTop = $(window).scrollTop();
           
               if (scrollTop > y_pos - height) {
                   $navbar.addClass("navbar-fixed")
               } else if (scrollTop <= y_pos) {
                   $navbar.removeClass("navbar-fixed")
               }
           });
           // Display docs subnav items
           $('.b-nav__docs').parent().toggleClass('nav__item__with__subs--expanded');
});
</script>

<!--#include virtual="../../streams/tutorial.html" -->
